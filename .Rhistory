y_train = y_combined[-test_id]
y_test = y_combined[test_id]
current_lr_fit = glm(bleached ~ clim_sst + temperature_kelvin + temperature_kelvin_standard_deviation + ssta_frequency + ssta_frequency_standard_deviation + tsa_frequency_standard_deviation + mean_cur, family = binomial, data = data)
predictions = predict(current_lr_fit, X_test)
cv_accuracy_folds[j] = accuracy(y_test, predictions)
}
cv_accuracy_overall[i] = mean(cv_accuracy_folds)
}
mean(cv_accuracy_gse1)
accuracy <- function(true, predicted) {
# Calculates accuracy based on a vector of true labels and a vector of predictions
return(sum(true == predicted)/length(true))
}
X_combined = data %>% select(-average_bleaching)
y_combined = data %>% pull(average_bleaching)
cvK = 5    # Number of CV folds
n_sim = 50 # Number of repeats
cv_accuracy_overall = numeric(n_sim) # Vector to store averaged CV accuracies
for (i in 1:n_sim) {
cvSets = cvFolds(nrow(data), cvK) # Folds object for cross-validation
cv_accuracy_folds = numeric(cvK) # Vector to store accuracy for each fold
for (j in 1:cvK) {
test_id = cvSets$subsets[cvSets$which == j]
# train = data[-test_id,] # Don't split training set to match glm syntax
X_train = X_combined[-test_id,]
X_test = X_combined[test_id,]
y_train = y_combined[-test_id]
y_test = y_combined[test_id]
current_lr_fit = glm(bleached ~ clim_sst + temperature_kelvin + temperature_kelvin_standard_deviation + ssta_frequency + ssta_frequency_standard_deviation + tsa_frequency_standard_deviation + mean_cur, family = binomial, data = data)
predictions = predict(current_lr_fit, X_test)
cv_accuracy_folds[j] = accuracy(y_test, predictions)
}
cv_accuracy_overall[i] = mean(cv_accuracy_folds)
}
mean(cv_accuracy_overall)
accuracy <- function(true, predicted) {
# Calculates accuracy based on a vector of true labels and a vector of predictions
return(sum(true == predicted)/length(true))
}
X_combined = data %>% select(-average_bleaching)
X_combined
y_combined = data %>% pull(average_bleaching)
y_combined
cvK = 5    # Number of CV folds
n_sim = 50 # Number of repeats
cv_accuracy_overall = numeric(n_sim) # Vector to store averaged CV accuracies
for (i in 1:n_sim) {
cvSets = cvFolds(nrow(data), cvK) # Folds object for cross-validation
cv_accuracy_folds = numeric(cvK) # Vector to store accuracy for each fold
for (j in 1:cvK) {
test_id = cvSets$subsets[cvSets$which == j]
# train = data[-test_id,] # Don't split training set to match glm syntax
X_train = X_combined[-test_id,]
X_test = X_combined[test_id,]
y_train = y_combined[-test_id]
y_test = y_combined[test_id]
current_lr_fit = glm(bleached ~ clim_sst + temperature_kelvin + temperature_kelvin_standard_deviation + ssta_frequency + ssta_frequency_standard_deviation + tsa_frequency_standard_deviation + mean_cur, family = binomial, data = data)
predictions = predict(current_lr_fit, X_test)
cv_accuracy_folds[j] = accuracy(y_test, predictions)
}
cv_accuracy_overall[i] = mean(cv_accuracy_folds)
}
mean(cv_accuracy_overall)
accuracy <- function(true, predicted) {
# Calculates accuracy based on a vector of true labels and a vector of predictions
return(sum(true == predicted)/length(true))
}
X_combined = data %>% select(-average_bleaching)
y_combined = data %>% pull(average_bleaching)
cvK = 5    # Number of CV folds
n_sim = 50 # Number of repeats
cv_accuracy_overall = numeric(n_sim) # Vector to store averaged CV accuracies
for (i in 1:n_sim) {
cvSets = cvFolds(nrow(data), cvK) # Folds object for cross-validation
cv_accuracy_folds = numeric(cvK) # Vector to store accuracy for each fold
for (j in 1:cvK) {
test_id = cvSets$subsets[cvSets$which == j]
# train = data[-test_id,] # Don't split training set to match glm syntax
X_train = X_combined[-test_id,]
X_test = X_combined[test_id,]
y_train = y_combined[-test_id]
y_test = y_combined[test_id]
current_lr_fit = glm(bleached ~ clim_sst + temperature_kelvin + temperature_kelvin_standard_deviation + ssta_frequency + ssta_frequency_standard_deviation + tsa_frequency_standard_deviation + mean_cur, family = binomial, data = data)
predictions = predict(current_lr_fit, X_test)
cv_accuracy_folds[j] = accuracy(y_test, predictions)
}
cv_accuracy_overall[i] = mean(cv_accuracy_folds)
}
mean(cv_accuracy_overall)
accuracy <- function(true, predicted) {
# Calculates accuracy based on a vector of true labels and a vector of predictions
return(sum(true == predicted)/length(true))
}
X_combined = data %>% select(-average_bleaching)
y_combined = data %>% pull(average_bleaching)
cvK = 5    # Number of CV folds
n_sim = 50 # Number of repeats
cv_accuracy_overall = numeric(n_sim) # Vector to store averaged CV accuracies
for (i in 1:n_sim) {
cvSets = cvFolds(nrow(data), cvK) # Folds object for cross-validation
cv_accuracy_folds = numeric(cvK) # Vector to store accuracy for each fold
for (j in 1:cvK) {
test_id = cvSets$subsets[cvSets$which == j]
# train = data[-test_id,] # Don't split training set to match glm syntax
X_train = X_combined[-test_id,]
X_test = X_combined[test_id,]
y_train = y_combined[-test_id]
y_test = y_combined[test_id]
current_lr_fit = glm(bleached ~ clim_sst + temperature_kelvin + temperature_kelvin_standard_deviation + ssta_frequency + ssta_frequency_standard_deviation + tsa_frequency_standard_deviation + mean_cur, family = binomial, data = data)
predictions = predict(current_lr_fit, X_test)
cv_accuracy_folds[j] = accuracy(y_test, predictions)
}
cv_accuracy_overall[i] = mean(cv_accuracy_folds)
}
round(mean(cv_accuracy_overall),2)
accuracy <- function(true, predicted) {
# Calculates accuracy based on a vector of true labels and a vector of predictions
return(sum(true == predicted)/length(true))
}
X_combined = data %>% select(-average_bleaching)
y_combined = data %>% pull(average_bleaching)
cvK = 5    # Number of CV folds
n_sim = 50 # Number of repeats
cv_accuracy_overall = numeric(n_sim) # Vector to store averaged CV accuracies
for (i in 1:n_sim) {
cvSets = cvFolds(nrow(data), cvK) # Folds object for cross-validation
cv_accuracy_folds = numeric(cvK) # Vector to store accuracy for each fold
for (j in 1:cvK) {
test_id = cvSets$subsets[cvSets$which == j]
train = data[-test_id,] # Don't split training set to match glm syntax
X_test = X_combined[test_id,]
y_test = y_combined[test_id]
current_lr_fit = glm(bleached ~ clim_sst + temperature_kelvin + temperature_kelvin_standard_deviation + ssta_frequency + ssta_frequency_standard_deviation + tsa_frequency_standard_deviation + mean_cur, family = binomial, data = data)
predicted_probs = predict(current_lr_fit, X_test,
type = "response") # Predicting probability of stable
predictions = ifelse(round(predicted_probs) == 1,
"Stable", "Rejection") # Converting probabilities into categories
cv_accuracy_folds[j] = accuracy(y_test, predictions)
}
cv_accuracy_overall[i] = mean(cv_accuracy_folds)
}
round(mean(cv_accuracy_overall),2)
round(mean(cv_accuracy_overall),2)
accuracy <- function(true, predicted) {
# Calculates accuracy based on a vector of true labels and a vector of predictions
return(sum(true == predicted)/length(true))
}
X_combined = data %>% select(-average_bleaching)
y_combined = data %>% pull(average_bleaching)
cvK = 5    # Number of CV folds
n_sim = 50 # Number of repeats
cv_accuracy_overall = numeric(n_sim) # Vector to store averaged CV accuracies
for (i in 1:n_sim) {
cvSets = cvFolds(nrow(data), cvK) # Folds object for cross-validation
cv_accuracy_folds = numeric(cvK) # Vector to store accuracy for each fold
for (j in 1:cvK) {
test_id = cvSets$subsets[cvSets$which == j]
train = data[-test_id,] # Don't split training set to match glm syntax
X_test = X_combined[test_id,]
y_test = y_combined[test_id]
current_lr_fit = glm(bleached ~ clim_sst + temperature_kelvin + temperature_kelvin_standard_deviation + ssta_frequency + ssta_frequency_standard_deviation + tsa_frequency_standard_deviation + mean_cur, family = binomial, data = data)
predicted_probs = predict(current_lr_fit, X_test,
type = "response") # Predicting probability of stable
predictions = ifelse(round(predicted_probs) == 1,
"Stable", "Rejection") # Converting probabilities into categories
cv_accuracy_folds[j] = accuracy(y_test, predictions)
}
cv_accuracy_overall[i] = mean(cv_accuracy_folds)
}
glm = glm(bleached ~ clim_sst + temperature_kelvin + temperature_kelvin_standard_deviation + ssta_frequency + ssta_frequency_standard_deviation + tsa_frequency_standard_deviation + mean_cur, family = binomial, data = data)
summary(glm)
glm
accuracy <- function(true, predicted) {
# Calculates accuracy based on a vector of true labels and a vector of predictions
return(sum(true == predicted)/length(true))
}
X_combined = data %>% select(-average_bleaching)
y_combined = data %>% pull(average_bleaching)
cvK = 5    # Number of CV folds
n_sim = 50 # Number of repeats
cv_accuracy_overall = numeric(n_sim) # Vector to store averaged CV accuracies
for (i in 1:n_sim) {
cvSets = cvFolds(nrow(data), cvK) # Folds object for cross-validation
cv_accuracy_folds = numeric(cvK) # Vector to store accuracy for each fold
for (j in 1:cvK) {
test_id = cvSets$subsets[cvSets$which == j]
train = data[-test_id,] # Don't split training set to match glm syntax
X_test = X_combined[test_id,]
y_test = y_combined[test_id]
current_lr_fit = glm(bleached ~ clim_sst + temperature_kelvin + temperature_kelvin_standard_deviation + ssta_frequency + ssta_frequency_standard_deviation + tsa_frequency_standard_deviation + mean_cur, family = binomial, data = data)
predicted_probs = predict(current_lr_fit, X_test,
type = "response") # Predicting probability of stable
predictions = ifelse(round(predicted_probs) == 1,
"Stable", "Rejection") # Converting probabilities into categories
cv_accuracy_folds[j] = accuracy(y_test, predictions)
}
cv_accuracy_overall[i] = mean(cv_accuracy_folds)
}
accuracy <- function(true, predicted) {
# Calculates accuracy based on a vector of true labels and a vector of predictions
return(sum(true == predicted)/length(true))
}
X_combined = data %>% select(-outcome)
knitr::opts_chunk$set(echo = TRUE)
library(ncdf4)
library(RNetCDF)
library(raster)
library(rgdal)
library(ggplot2)
library(tidyverse)
library(dplyr)
library(rasterVis)
library(sf)
library(terra)
load("data/gse_GSE46474.RData"); gse4 = gse_GSE46474
load("data/GSE46474.RData"); gse4 = gse_GSE46474
load("data/GSE46474.RData")
load("data/GSE46474.RData")
gse4 = gse_GSE46474
load("data/GSE46474.RData")
gse4 = GSE46474
gse4 = load("data/GSE46474.RData")
combined_df = clinical_df %>%
mutate(apv = as.factor(apv),
outcome = as.factor(y))
clinical_df = pData(gse4) %>%
select(c("age_tx:ch1", "Sex:ch1")) %>%
rename("age" = "age_tx:ch1",
"sex" = "Sex:ch1") %>%
mutate(age = as.numeric(age),
sex = as.factor(sex))
gse4 = load("data/gse_GSE46474.RData")
clinical_df = pData(gse4) %>%
select(c("age_tx:ch1", "Sex:ch1")) %>%
rename("age" = "age_tx:ch1",
"sex" = "Sex:ch1") %>%
mutate(age = as.numeric(age),
sex = as.factor(sex))
gse4 = load("data/gse_GSE46474.RData")
gse4
load("data/gse_GSE46474.RData"); gse4 = gse_GSE46474
gse4
clinical_df = pData(gse4) %>%
select(c("age_tx:ch1", "Sex:ch1")) %>%
rename("age" = "age_tx:ch1",
"sex" = "Sex:ch1") %>%
mutate(age = as.numeric(age),
sex = as.factor(sex))
clinical_df = pData(gse4) %>%
select(c("age_tx:ch1", "Sex:ch1")) %>%
rename("age" = "age_tx:ch1",
"sex" = "Sex:ch1") %>%
mutate(age = as.numeric(age),
sex = as.factor(sex))
clinical_df = pData(gse4) %>%
select(c("age_tx:ch1", "Sex:ch1")) %>%
rename("age" = "age_tx:ch1",
"sex" = "Sex:ch1") %>%
mutate(age = as.numeric(age),
sex = as.factor(sex))
accuracy <- function(true, predicted) {
# Calculates accuracy based on a vector of true labels and a vector of predictions
return(sum(true == predicted)/length(true))
}
X_combined = data %>% select(-average_bleaching)
knitr::opts_chunk$set(echo = TRUE)
library(tidyr)
library(dplyr)
library(tidyverse)
library(sjPlot)
library(cvTools)
library(vcdExtra)
library(corrplot)
library(RColorBrewer)
knitr::opts_chunk$set(echo = TRUE)
library(tidyr)
library(dplyr)
library(tidyverse)
library(sjPlot)
library(cvTools)
library(vcdExtra)
library(corrplot)
library(RColorBrewer)
reef = read_csv("Data/Reef_Check_with_cortad_variables_with_annual_rate_of_SST_change.csv")
new_reef = read_csv("Data/lr_version_merged_mean.csv")
# Check what the reef dataset looks like
head(new_reef)
## clean variable names
data = new_reef %>% janitor::clean_names()
names(data)
## removing NA data entries
data <- na.omit(data)
## changing date to date
# data$date <- as.Date(data$date, format = "%d-%b-%y")
## changing temperature to celcius
data$temperature_celcius <- data$temperature_kelvin - 273.15
## longitude and latitude maximum and minimum to match dataset
# We have to sort out bleached and not bleached corals
data$bleached <- ifelse(data$average_bleaching > 0, "1", "0")
# 0 = not bleached
# 1 = bleached
data %>% count(bleached)
# Convert to numeric
data$bleached <- as.numeric(data$bleached)
glm = glm(bleached ~ clim_sst + temperature_kelvin + temperature_kelvin_standard_deviation + ssta_frequency + ssta_frequency_standard_deviation + tsa_frequency_standard_deviation + mean_cur, family = binomial, data = data)
summary(glm)
glm
accuracy <- function(true, predicted) {
# Calculates accuracy based on a vector of true labels and a vector of predictions
return(sum(true == predicted)/length(true))
}
X_combined = data %>% select(-average_bleaching)
y_combined = data %>% pull(average_bleaching)
cvK = 5    # Number of CV folds
n_sim = 50 # Number of repeats
cv_accuracy_overall = numeric(n_sim) # Vector to store averaged CV accuracies
for (i in 1:n_sim) {
cvSets = cvFolds(nrow(data), cvK) # Folds object for cross-validation
cv_accuracy_folds = numeric(cvK) # Vector to store accuracy for each fold
for (j in 1:cvK) {
test_id = cvSets$subsets[cvSets$which == j]
train = data[-test_id,] # Don't split training set to match glm syntax
X_test = X_combined[test_id,]
y_test = y_combined[test_id]
current_lr_fit = glm(bleached ~ clim_sst + temperature_kelvin + temperature_kelvin_standard_deviation + ssta_frequency + ssta_frequency_standard_deviation + tsa_frequency_standard_deviation + mean_cur, family = binomial, data = data)
predicted_probs = predict(current_lr_fit, X_test,
type = "response") # Predicting probability of stable
predictions = ifelse(round(predicted_probs) == 1,
"Stable", "Rejection") # Converting probabilities into categories
cv_accuracy_folds[j] = accuracy(y_test, predictions)
}
cv_accuracy_overall[i] = mean(cv_accuracy_folds)
}
round(mean(cv_accuracy_overall),2)
accuracy <- function(true, predicted) {
# Calculates accuracy based on a vector of true labels and a vector of predictions
return(sum(true == predicted)/length(true))
}
X_combined = data %>% select(-average_bleaching)
y_combined = data %>% pull(average_bleaching)
cvK = 5    # Number of CV folds
n_sim = 50 # Number of repeats
cv_accuracy_overall = numeric(n_sim) # Vector to store averaged CV accuracies
for (i in 1:n_sim) {
cvSets = cvFolds(nrow(data), cvK) # Folds object for cross-validation
cv_accuracy_folds = numeric(cvK) # Vector to store accuracy for each fold
for (j in 1:cvK) {
test_id = cvSets$subsets[cvSets$which == j]
train = data[-test_id,] # Don't split training set to match glm syntax
X_test = X_combined[test_id,]
y_test = y_combined[test_id]
current_lr_fit = glm(bleached ~ clim_sst + temperature_kelvin + temperature_kelvin_standard_deviation + ssta_frequency + ssta_frequency_standard_deviation + tsa_frequency_standard_deviation + mean_cur, family = binomial, data = data)
predicted_probs = predict(current_lr_fit, X_test,
type = "response") # Predicting probability of stable
predictions = ifelse(round(predicted_probs) == 1,
"Stable", "Rejection") # Converting probabilities into categories
cv_accuracy_folds[j] = accuracy(y_test, predictions)
}
cv_accuracy_overall[i] = mean(cv_accuracy_folds)
}
knitr::opts_chunk$set(echo = TRUE)
library(tidyr)
library(dplyr)
library(tidyverse)
library(sjPlot)
library(cvTools)
library(vcdExtra)
library(corrplot)
library(RColorBrewer)
library(caret)
# define training control
train_control <- trainControl(method = "cv", number = 10)
# train the model on training set
model <- train(target ~ .,
data = train,
trControl = train_control,
method = "glm",
family=binomial())
# define training control
train_control <- trainControl(method = "cv", number = 10)
# train the model on training set
model <- train(average_bleaching ~ .,
data = data,
trControl = train_control,
method = "glm",
family=binomial())
# define training control
train_control <- trainControl(method = "cv", number = 10)
# train the model on training set
model <- train(bleached ~ clim_sst + temperature_kelvin + temperature_kelvin_standard_deviation + ssta_frequency + ssta_frequency_standard_deviation + tsa_frequency_standard_deviation + mean_cur,
data = data,
method = "glm",
trControl = train_control,
family=binomial())
# print cv scores
summary(model)
accuracy <- function(true, predicted) {
# Calculates accuracy based on a vector of true labels and a vector of predictions
return(sum(true == predicted)/length(true))
}
X_combined = data %>% select(-bleached)
y_combined = data %>% pull(bleached)
cvK = 5    # Number of CV folds
n_sim = 50 # Number of repeats
cv_accuracy_overall = numeric(n_sim) # Vector to store averaged CV accuracies
for (i in 1:n_sim) {
cvSets = cvFolds(nrow(data), cvK) # Folds object for cross-validation
cv_accuracy_folds = numeric(cvK) # Vector to store accuracy for each fold
for (j in 1:cvK) {
test_id = cvSets$subsets[cvSets$which == j]
train = data[-test_id,] # Don't split training set to match glm syntax
X_test = X_combined[test_id,]
y_test = y_combined[test_id]
current_lr_fit = glm(bleached ~ clim_sst + temperature_kelvin + temperature_kelvin_standard_deviation + ssta_frequency + ssta_frequency_standard_deviation + tsa_frequency_standard_deviation + mean_cur, family = binomial, data = data)
predicted_probs = predict(current_lr_fit, X_test,
type = "response") # Predicting probability of stable
predictions = ifelse(round(predicted_probs) == 1,
"Stable", "Rejection") # Converting probabilities into categories
cv_accuracy_folds[j] = accuracy(y_test, predictions)
}
cv_accuracy_overall[i] = mean(cv_accuracy_folds)
}
round(mean(cv_accuracy_overall),2)
# define training control
train_control <- trainControl(method = "cv", number = 10)
# train the model on training set
model <- train(bleached ~ clim_sst + temperature_kelvin + temperature_kelvin_standard_deviation + ssta_frequency + ssta_frequency_standard_deviation + tsa_frequency_standard_deviation + mean_cur,
data = data,
method = "glm",
trControl = train_control,
family=binomial())
# print cv scores
summary(model)
# define training control
train_control <- trainControl(method = "cv", number = 10)
# train the model on training set
model <- train(bleached ~ clim_sst + temperature_kelvin + temperature_kelvin_standard_deviation + ssta_frequency + ssta_frequency_standard_deviation + tsa_frequency_standard_deviation + mean_cur,
data = data,
method = "glm",
trControl = train_control,
family=binomial())
# print cv scores
print(model)
# define training control
train_control <- trainControl(method = "cv", number = 10)
# train the model on training set
model <- train(bleached ~ clim_sst + temperature_kelvin + temperature_kelvin_standard_deviation + ssta_frequency + ssta_frequency_standard_deviation + tsa_frequency_standard_deviation + mean_cur,
data = data,
method = "glm",
trControl = train_control,
family=binomial())
# print cv scores
print(model)
model$resample
# define training control
train_control <- trainControl(method = "cv", number = 10)
# train the model on training set
model <- train(bleached ~ clim_sst + temperature_kelvin + temperature_kelvin_standard_deviation + ssta_frequency + ssta_frequency_standard_deviation + tsa_frequency_standard_deviation + mean_cur,
data = data,
method = "glm",
trControl = train_control,
family=binomial())
# print cv scores
print(model)
model
model$resample
pred <- model$pred
pred$equal <- ifelse(pred$pred == pred$obs, 1,0)
eachfold <- pred %>%
group_by(Resample) %>%
summarise_at(vars(equal),
list(Accuracy = mean))
# define training control
train_control <- trainControl(method = "cv", number = 10)
# train the model on training set
model <- train(bleached ~ clim_sst + temperature_kelvin + temperature_kelvin_standard_deviation + ssta_frequency + ssta_frequency_standard_deviation + tsa_frequency_standard_deviation + mean_cur,
data = data,
method = "glm",
trControl = train_control,
family=binomial())
# print cv scores
print(model)
model$resample
round(mean(model),2)
# define training control
train_control <- trainControl(method = "cv", number = 10)
# train the model on training set
model <- train(bleached ~ clim_sst + temperature_kelvin + temperature_kelvin_standard_deviation + ssta_frequency + ssta_frequency_standard_deviation + tsa_frequency_standard_deviation + mean_cur,
data = data,
method = "glm",
trControl = train_control,
family=binomial())
# print cv scores
print(model)
model$resample
mean(model)
# define training control
train_control <- trainControl(method = "cv", number = 10, savePredictions = TRUE)
# train the model on training set
model <- train(bleached ~ clim_sst + temperature_kelvin + temperature_kelvin_standard_deviation + ssta_frequency + ssta_frequency_standard_deviation + tsa_frequency_standard_deviation + mean_cur,
data = data,
method = "glm",
trControl = train_control,
family=binomial())
# print cv scores
print(model)
model$resample
pred <- predict(model, newdata=testing)
pred <- predict(model, newdata=train)
confusionMatrix(data=pred, testing$Class)
pred <- predict(model, newdata=train)
confusionMatrix(data=pred, train$Class)
