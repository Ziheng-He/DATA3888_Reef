---
title: "Regression"
output: html_document
date: '2022-05-04'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyr)
library(dplyr)
library(tidyverse)

library(sjPlot)
library(cvTools)

library(vcdExtra)
library(corrplot)
library(RColorBrewer)
library(caret)
```

Reading in the reef data:
```{r}
#reef = read_csv("Data/Reef_Check_with_cortad_variables_with_annual_rate_of_SST_change.csv")

new_reef = read_csv("Data_copy/lr_version_merged_mean.csv")

# Check what the reef dataset looks like
head(new_reef)
```
Data cleaning: 

- Working on the copy of the reef dataset called data
- Cleaned names so it would be easier to work with the variables

```{r}
## clean variable names
data = new_reef %>% janitor::clean_names()

names(data)
```
- Removing NA rows, we cannot work with missing values that we cannot access from the dataset
- Converting format of entries in the variables we can use
```{r}
## removing NA data entries
data <- na.omit(data)

## changing date to date
# data$date <- as.Date(data$date, format = "%d-%b-%y")

## changing temperature to celcius
data$temperature_celcius <- data$temperature_kelvin - 273.15

## longitude and latitude maximum and minimum to match dataset
```

Assumption: For the logistic regression, the predictors are meant to be binary. So we convert the entries of average_bleaching >0 to be 1.
```{r}
# We have to sort out bleached and not bleached corals
data$bleached <- ifelse(data$average_bleaching > 0, "1", "0")
# 0 = not bleached
# 1 = bleached
data %>% count(bleached)

# Convert to numeric
data$bleached <- as.factor(data$bleached)

```

Assumption: Observations from the dataset can be related to each other?

Assumption: Correlation 


Fitting the Logistic Regression:

```{r}
glm = glm(bleached ~ clim_sst + temperature_kelvin + temperature_kelvin_standard_deviation + ssta_frequency + ssta_frequency_standard_deviation + tsa_frequency_standard_deviation + mean_cur, family = binomial, data = data)
summary(glm)

glm
```
Predicting accuracy

accuracy
- RMSE by CV (lower is better)
The RMSE calculated by cross validation was 0.3508347. 

stability
- how different model performs across all models
- spread of rmse (boxplot), spread across cv
We want lower RMSE, high Rsquared and lower MAE to indicated how closely the model can predict actual observations.
  
scalability
- how well it scales across different sizes
- time code in r? how long models take to run (10%, 20, 50, 100)
how steep the line is tells u how scalable the model is

compare the models

interpretability
- something u need to say about the model
We can easily interpret the predictions of model through an equation from the coefficients


```{r}
## gets 0?
accuracy <- function(true, predicted) {
  # Calculates accuracy based on a vector of true labels and a vector of predictions
  return(sum(true == predicted)/length(true))
}

X_combined = data %>% select(-bleached)
y_combined = data %>% pull(bleached)

cvK = 5    # Number of CV folds
n_sim = 50 # Number of repeats
cv_accuracy_overall = numeric(n_sim) # Vector to store averaged CV accuracies

for (i in 1:n_sim) {
  
  cvSets = cvFolds(nrow(data), cvK) # Folds object for cross-validation
  cv_accuracy_folds = numeric(cvK) # Vector to store accuracy for each fold
  
  for (j in 1:cvK) {
    
    test_id = cvSets$subsets[cvSets$which == j]
    train = data[-test_id,] # Don't split training set to match glm syntax
    X_test = X_combined[test_id,]
    y_test = y_combined[test_id]
    
    current_lr_fit = glm(bleached ~ clim_sst + temperature_kelvin + temperature_kelvin_standard_deviation + ssta_frequency + ssta_frequency_standard_deviation + tsa_frequency_standard_deviation + mean_cur, family = binomial, data = data)
    
    predicted_probs = predict(current_lr_fit, X_test, 
                              type = "response") # Predicting probability of stable
    predictions = ifelse(round(predicted_probs) == 1, 
                         "Stable", "Rejection") # Converting probabilities into categories
    cv_accuracy_folds[j] = accuracy(y_test, predictions)
    
  }
  
  cv_accuracy_overall[i] = mean(cv_accuracy_folds)
  
}
```

```{r}
round(mean(cv_accuracy_overall),2)
```

Using caret:
```{r}
# define training control
train_control <- trainControl(method = "cv", number = 10, savePredictions = TRUE)

# train the model on training set
model <- train(bleached ~ clim_sst + temperature_kelvin + temperature_kelvin_standard_deviation + ssta_frequency + ssta_frequency_standard_deviation + tsa_frequency_standard_deviation + mean_cur,
               data = data,
               method = "glm",
               trControl = train_control,
               family=binomial())

# print cv scores
print(model)

accuracy_folds <- model$resample[1]


```
Showing Stability in a boxplot:
```{r}
boxplot(accuracy_folds) + geom_jitter()
```



Scalability:
```{r}
#Sampling 10% of the data
sample_10 <- data[sample(1:nrow(data), 27,
   replace=FALSE),]

start_time_10 <- Sys.time()

glm = glm(bleached ~ clim_sst + temperature_kelvin + temperature_kelvin_standard_deviation + ssta_frequency + ssta_frequency_standard_deviation + tsa_frequency_standard_deviation + mean_cur, family = binomial, data = sample_10)

end_time_10 <- Sys.time()

time_10 = end_time_10 - start_time_10
```

```{r}
#sampling 50% of data
sample_50 <- data[sample(1:nrow(data), 135,
   replace=FALSE),]

start_time_50 <- Sys.time()

glm = glm(bleached ~ clim_sst + temperature_kelvin + temperature_kelvin_standard_deviation + ssta_frequency + ssta_frequency_standard_deviation + tsa_frequency_standard_deviation + mean_cur, family = binomial, data = sample_50)

end_time_50 <- Sys.time()

time_50 = end_time_50 - start_time_50
```

```{r}
#sampling 70% of data
sample_70 <- data[sample(1:nrow(data), 189,
   replace=FALSE),]

start_time_70 <- Sys.time()

glm = glm(bleached ~ clim_sst + temperature_kelvin + temperature_kelvin_standard_deviation + ssta_frequency + ssta_frequency_standard_deviation + tsa_frequency_standard_deviation + mean_cur, family = binomial, data = sample_70)

end_time_70 <- Sys.time()

time_70 = end_time_70 - start_time_70
```

```{r}
#the whole data
start_time_100 <- Sys.time()

glm = glm(bleached ~ clim_sst + temperature_kelvin + temperature_kelvin_standard_deviation + ssta_frequency + ssta_frequency_standard_deviation + tsa_frequency_standard_deviation + mean_cur, family = binomial, data = data)

end_time_100 <- Sys.time()

time_100 = end_time_100 - start_time_100

```
With our dataset, it doesn't seem like the model will take more than 1 second.

Modeling scalability in a line graph:
```{r}
time_df = data.frame(Sample_percent = c(10,50,70,100),
                Time = c(time_10, time_50, time_70, time_100))

#export to Zhenyu, convert to csv
write.csv(time_df, "time_df.csv", row.names=FALSE)
```


```{r}
path = "./Models/logistic"
model_rds_path = paste(path, ".rds",sep='')
model_dep_path = paste(path, ".dep",sep='')

DEP_LIBS = c("caret")

# save model
saveRDS(model, model_rds_path)


# save dependency list
file_conn <- file(model_dep_path)
writeLines("",file_conn)
close(file_conn)
```


