---
title: "Regression"
output: html_document
date: '2022-05-04'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyr)
library(dplyr)
library(tidyverse)
library(maps)
library(ggplot2)

library(sjPlot)
library(cvTools)
library(ozmaps)

library(vcdExtra)
library(corrplot)
library(RColorBrewer)
library(caret)
require(gridExtra)

```

Reading in the reef data:
```{r}
#reef = read_csv("Data/Reef_Check_with_cortad_variables_with_annual_rate_of_SST_change.csv")

new_reef = read_csv("Data_copy/lr_version_merged_mean.csv")

# Check what the reef dataset looks like
head(new_reef)
```
Data cleaning: 

- Working on the copy of the reef dataset called data
- Cleaned names so it would be easier to work with the variables

```{r}
## clean variable names
data = new_reef %>% janitor::clean_names()

names(data)
```


- Removing NA rows, we cannot work with missing values that we cannot access from the dataset
- Converting format of entries in the variables we can use

```{r}
## removing NA data entries
data <- na.omit(data)

## changing date to date
# data$date <- as.Date(data$date, format = "%d-%b-%y")

## changing temperature to celcius
data$temperature_celcius <- data$temperature_kelvin - 273.15

## longitude and latitude maximum and minimum to match dataset
```


Assumption: For the logistic regression, the predictors are meant to be binary. So we convert the entries of average_bleaching >0 to be 1.
```{r}
# We have to sort out bleached and not bleached corals
data$bleached <- ifelse(data$average_bleaching > 0, "1", "0")
# 0 = not bleached
# 1 = bleached
data %>% count(bleached)

# Convert to numeric
data$bleached <- as.factor(data$bleached)
```


map visualisation
```{r}
world <- map_data("world")
oz_states <- ozmap_states

ggplot(oz_states) + geom_sf() + coord_sf() +
  geom_point(
    data = data,
    aes(reef_longitude, reef_latitude, colour = bleached),
    alpha = 0.7) + ggtitle("Amount of coral points bleached vs not bleached in the GBF") + 
  ylab("Latitude") + xlab("Longitude") + scale_colour_manual(labels = c("not bleached", "bleached"), values = c("red", "blue"))

oz_states <- ozmap_states

ggplot(oz_states) + geom_sf() + coord_sf() +
  geom_point(
    data = data,
    aes(reef_longitude, reef_latitude, size = average_bleaching, colour = average_bleaching),
    alpha = 0.7) + ggtitle("Average coral bleaching along the GBF") + 
  ylab("Latitude") + xlab("Longitude") 

```

Assumption: Observations from the dataset can be related to each other?

Assumption: Correlation 
```{r, include = FALSE, eval = FALSE}
# Split the data into training and test set
set.seed(123)
training.samples <- data$bleached %>% 
  createDataPartition(p = 0.8, list = FALSE)
train.data  <- data[training.samples, ]
test.data <- data[-training.samples, ]

# Fit the model
model <- glm( bleached ~ clim_sst + temperature_kelvin + temperature_kelvin_standard_deviation + ssta_frequency + ssta_frequency_standard_deviation + tsa_frequency_standard_deviation + mean_cur, data = train.data, family = binomial)
# Summarize the model
summary(model)
# Make predictions
probabilities <- model %>% predict(test.data, type = "response")
predicted.classes <- ifelse(probabilities > 0, "pos", "neg")
# Model accuracy
mean <- mean(predicted.classes == test.data$bleached)
mean
```

```{r}
data %>%
  ggplot(mapping = aes(x = mean_cur, y = average_bleaching)) +
  geom_point(aes(colour = bleached)) +
  geom_smooth(method = lm)
```

```{r}

```



do i use train data or whole data??
```{r}
# for model prediction
model <- glm(bleached ~ mean_cur, data = train.data, family = binomial)
summary(model)$coef


# to see if variables are significant
model2 <- glm(bleached ~ mean_cur, data = data, family = binomial)
summary(model2)$coef

```



Fitting the Logistic Regression:

```{r}
glm = glm(bleached ~ clim_sst + temperature_kelvin + temperature_kelvin_standard_deviation + ssta_frequency + ssta_frequency_standard_deviation + tsa_frequency_standard_deviation + mean_cur, family = binomial, data = data)
summary(glm)

glm
```

trying to graph model:

```{r, eval = FALSE}
fit <- glm(bleached ~ mean_cur, family = binomial, data = data)
fit
ggplot(data, aes(x=mean_cur, y=as.numeric(as.character(bleached)))) + 
  geom_point() +
  stat_smooth(method="glm", se=FALSE, method.args = list(family="binomial"))
```


Predicting accuracy

accuracy
- RMSE by CV (lower is better)
The RMSE calculated by cross validation was 0.3508347. 

stability
- how different model performs across all models
- spread of rmse (boxplot), spread across cv
We want lower RMSE, high Rsquared and lower MAE to indicated how closely the model can predict actual observations.
  
scalability
- how well it scales across different sizes
- time code in r? how long models take to run (10%, 20, 50, 100)
how steep the line is tells u how scalable the model is

compare the models

interpretability
- something u need to say about the model
We can easily interpret the predictions of model through an equation from the coefficients


Using caret for accuracy:
```{r}
# define training control
set.seed(1)
train_control <- trainControl(method = "cv", number = 5, savePredictions = TRUE)

# train the model on training set
model <- train(bleached ~ clim_sst + temperature_kelvin + temperature_kelvin_standard_deviation + ssta_frequency + ssta_frequency_standard_deviation + tsa_frequency_standard_deviation + mean_cur,
               data = data,
               method = "glm",
               trControl = train_control,
               family=binomial())

# print cv scores
print(model$results[2])


model$resample

accuracy_folds <- model$resample[1]


```
Showing Stability in a boxplot:
```{r}
accuracy_folds 

boxplot(accuracy_folds) 
```
Compare accuracy with:
```{r}
#1:
data %>% count(bleached)
data_accuracy_score = 226/(226+45)
data_accuracy_score

#2:
model_accuracy = model$results[[2]]
model_accuracy
```




Scalability:
```{r}
#Sampling 10% of the data
sample_10 <- data[sample(1:nrow(data), 27,
   replace=FALSE),]

start_time_10 <- Sys.time()

glm = glm(bleached ~ clim_sst + temperature_kelvin + temperature_kelvin_standard_deviation + ssta_frequency + ssta_frequency_standard_deviation + tsa_frequency_standard_deviation + mean_cur, family = binomial, data = sample_10)

end_time_10 <- Sys.time()

time_10 = end_time_10 - start_time_10
```

```{r}
#sampling 50% of data
sample_50 <- data[sample(1:nrow(data), 135,
   replace=FALSE),]

start_time_50 <- Sys.time()

glm = glm(bleached ~ clim_sst + temperature_kelvin + temperature_kelvin_standard_deviation + ssta_frequency + ssta_frequency_standard_deviation + tsa_frequency_standard_deviation + mean_cur, family = binomial, data = sample_50)

end_time_50 <- Sys.time()

time_50 = end_time_50 - start_time_50
```

```{r}
#sampling 70% of data
sample_70 <- data[sample(1:nrow(data), 189,
   replace=FALSE),]

start_time_70 <- Sys.time()

glm = glm(bleached ~ clim_sst + temperature_kelvin + temperature_kelvin_standard_deviation + ssta_frequency + ssta_frequency_standard_deviation + tsa_frequency_standard_deviation + mean_cur, family = binomial, data = sample_70)

end_time_70 <- Sys.time()

time_70 = end_time_70 - start_time_70
```

```{r}
#the whole data
start_time_100 <- Sys.time()

glm = glm(bleached ~ clim_sst + temperature_kelvin + temperature_kelvin_standard_deviation + ssta_frequency + ssta_frequency_standard_deviation + tsa_frequency_standard_deviation + mean_cur, family = binomial, data = data)

end_time_100 <- Sys.time()

time_100 = end_time_100 - start_time_100

```
With our dataset, it doesn't seem like the model will take more than 1 second.

Modeling scalability in a line graph:
```{r}
time_df = data.frame(
  Sample_percent = c(10,50,70,100),
  Time = c(time_10, time_50, time_70, time_100))

write.csv(time_df, "time_df.csv", row.names=FALSE)

write.table(model$results[2], "time_df.csv",
            append = TRUE,
            sep = ",",
            col.names = "accuracy",
            row.names = TRUE,
            quote = FALSE)

write.table(accuracy_folds[[1]], "time_df.csv",
            append = TRUE,
            sep = ",",
            col.names = "acc_5_f",
            row.names = TRUE,
            quote = FALSE)
```

```{r}
sample_p = c(10,50,70,100)
time_p = c(time_10, time_50, time_70, time_100)
acc = model$results[2]
acc_5_f <- accuracy_folds[[1]]

length(sample_p) <- length(acc_5_f)
length(time_p) <- length(acc_5_f)
length(acc) <- length(acc_5_f)

write.table(cbind(sample_p,time_p,acc,acc_5_f), 
file="lr.csv",row.names=F,col.names=c('sample_percent','time', 'accuracy','accuracy_5_folds'))
```



```{r, eval = FALSE}
path = "./Models/logistic"
model_rds_path = paste(path, ".rds",sep='')
model_dep_path = paste(path, ".dep",sep='')

DEP_LIBS = c("caret")

# save model
saveRDS(model, model_rds_path)


# save dependency list
file_conn <- file(model_dep_path)
writeLines("",file_conn)
close(file_conn)
```

- there is more bleached points than unbleached points in the great barrier reef
- logistic regression isnt a good presentation of the log odds of this data. as the accuracy is 82% lower than 83%(which was proportion of bleached/total data). Since RF has highest accuracy, we will be using that one.
